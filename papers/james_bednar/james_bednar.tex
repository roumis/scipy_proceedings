% Up to 8 pages formatted

\subsection{Introduction \label{introduction}}

Modern science, engineering, and analysis relies on computational tools for working with data, such as the foundational NumPy \cite{harris:nature20}, Pandas \cite{mckinney:scipy10}, and Matplotlib \cite{hunter:cse07} libraries for Python. Over the past few decades, many different research areas and communities have built their own ``stacks'', i.e. layered sets of software tools that are combined to solve problems in that particular research area. For instance, a Python geoscience stack might combine GDAL and Fiona for geoscience file-format access, Xarray (itself built on NumPy) for multidimensional array processing, cartopy (itself built on PROJ and NumPy) for handling earth coordinates, Matplotlib for plotting, and Jupyter for user interaction and code execution. A Python financial analysis stack might combine Pandas for file reading and columnar data manipulation, Matplotlib for plotting, TA-Lib for financial mathematics functions, and Jupyter for user interaction and code execution.

Many of the components of these stacks date back decades before Python became popular, capturing important functionality but inheriting technical complexity and limitations that may no longer apply. For instance, domain-specific visualization and user interface (UI) tools are often tied to a local desktop operating system and graphical user interface (GUI), limiting the stack to working with data and processing power available locally, and making it difficult to share work with colleagues at other sites or using other operating systems. Older tools are often either inherently single threaded, with no support for distributed computation, or specifically focused on supercomputing systems rather than flexibly supporting the diversity of computing platforms now available (such as GPUs and cloud computing). Software is of course infinitely malleable, and so any such limitations could be addressed \emph{in principle}, but in practice each tool has a relatively narrow collection of users, potential contributors, and funding sources for that domain, limiting the scope of such development.

Could there be a better way? Yes! Modern Python includes general data-processing tools that address tasks that cross research areas, domains, and industries, focusing on storing, reading, processing, plotting, analyzing, modeling, and exploring data. This paper introduces a collection of such tools called SOSA: the Scalable Open-Source Analysis Stack.

SOSA tools are maintained independently and are usable individually, but together they form a solid basis for processing data of almost any kind. The SOSA Python libraries are:

\begin{itemize}
  \item\textbf{Domain independent:} Maintained, used, and tested by people from many different backgrounds
  \item\textbf{Efficient:} Run at machine-code speeds using vectorized data or JIT compilation
  \item\textbf{Scalable:} Run on anything from a single-core laptop to a thousand-node cluster
  \item\textbf{Cloud friendly:} Fully usable for local or remote compute using data on any file storage system
  \item\textbf{Multi-architecture:} Run on your desktop and on Mac/Windows/Linux CPUs and GPUs
  \item\textbf{Scriptable:} Run in batch mode for parameter searches and unattended operation
  \item\textbf{Compositional:} Select which tools you need and put them together to solve your problem
  \item\textbf{Visualizable:} Support rendering even the largest datasets without conversion or approximation
  \item\textbf{Interactive:} Support fully interactive exploration, not just rendering static images or text files
  \item\textbf{Shareable:} Deployable as web apps for use by anyone anywhere
  \item\textbf{OSS:} Free, open, and ready for research or commercial use, without restrictive licensing
\end{itemize}

\begin{figure*}[t]
    \noindent\makebox[\textwidth][c]{\includegraphics[width=\textwidth]{sosa.png}}
    \caption{SOSA: The Scalable Open-Source Analysis Stack\label{sosa}}
\end{figure*}
%%
Figure \ref{sosa} lists the libraries that make up SOSA, including parquet, fsspec, kerchunk, pandas, xarray, rapids, dask, numba, hvplot, panel, and jupyter, putting them into categories that will be described fully in later sections. Each of these tools is maintained separately and usable independently, but here we argue that the collection of them together is a meaningful base for general-purpose scientific computation across research areas and scientific domains. We will first look at the features that governed the selection of these particular libraries and that in many cases drive development priorities in the library itself. We will then describe the specific libraries involved and how they can be put together by users to solve computing tasks and how tool authors can build on them to support scientific computing in their own discipline.
% parquet is not a library. At least don't put it first!

\subsection{Design considerations}

Library authors have to choose between different designs and alternative technologies when deciding which features are implemented and how they are built.  Here we will unpack each of the above considerations driving the SOSA collection of libraries and examine how these libraries implement those principles.

\subsubsection{Domain independent} SOSA tools are all designed to be applied across scientific domains, without making design choices that constrain them to a narrow subfield or topic. At first it may seem that having something written specifically for your particular research area would be ideal, but we argue that it is neither desirable nor feasible to draw strict boundaries between active scientific research areas. Should you have to switch to an entirely different stack whenever you do collaborative work? What if you just want to do something novel, which differs slightly from assumptions previously made in your research area? If your stack is deep and difficult to change while encoding a fixed and unnecessarily brittle model of your domain, scientific progress will be difficult to achieve. Of course, not every aspect of research crosses multiple domains or challenges core assumptions, but using SOSA tools ensures that doing so will come at relatively little cost, because the same tools are valid across a very wide range of fields. SOSA tools can also draw from expertise and funding across all of science and analysis, rather than from a single narror area.

\subsubsection{Efficient} Because Python itself is a relatively slow, interpreted language, native Python code is often suitable only for relatively small problems, limiting the audience for any tool built purely in Python with Python dependencies. Larger problems have historically been addressed by wrapping fast code written in compiled languages like FORTRAN and C. However, this compiled code often comes with limitations such as being either single threaded or else tied to specific multithreading technologies like MPI, as well as often hard-coding assumptions specific to a certain research area. So that the same tools are applicable across a wide range of problems, SOSA libraries typically use the Numba Python JIT compiler to compile Python syntax into machine code that runs as fast as C or Rust or FORTRAN.
% I wouldn't say that Numba is "typical". numpy, pandas and many others use native extensions and cython, and make up far more of the
% code base than anything in Numba. Numba is great for allowing fast executino of *novel* numeric python code. Multithreading with Rust is
% very easy, and would distribute just fine with Dask.

\subsubsection{Scalable} Even compiled code is not sufficient to address the largest problems, which require more memory than is available on a single machine, or require long computation that is feasible only when split across many processors working simultaneously. To cover all these cases, SOSA tools support distributed computation using Dask on hundreds or thousands of processors, while also fully supporting a single laptop or workstation. That way there is no need to switch to a different stack when you hit a problem larger than your machine, or when you want to work on small problems that do not need extensive infrastructure. Supporting distributed computation efficiently requires tooling at every level, starting with chunked binary file formats like Parquet and Zarr and culminating in visualization tools like Datashader that can render plots in separate chunks on each processor and combine them for the final display.

\subsubsection{Cloud friendly}
When working with data or compute at remote locations like a cloud compute server, it is often infeasible to copy data locally for display, and so it is crucial to have visualization tools that support remote access. SOSA tools focus on browser-based visualization tools that let you work the same whether the computation happens on a laptop or on a server in the cloud. SOSA tools also support efficient and flexible access to data wherever it might be located, using fsspec to provide uniform filesystem-like access for data on local disk, cloud storage, web servers, and many other locations.
% I think the biggest point is, onlyaccessing the bytes that you need for your analysis (as far as possible). The previous model was either
% to download all of a dataset whether you needed it or not, or have some online query tool to fetch files matching your query
% The other part is compute->data, which means cloud deployments of dask and/or j-hubs.

\subsubsection{Multi-architecture}
Because researchers typically use Windows or Mac systems locally while cloud servers typically run Linux, research code needs to be independent of the operating system for it to be equally well supported on local and remote systems. A software stack that is tied to a particular OS not only shuts out users who are not on that OS, it often restricts usage to either cloud or local usage, further reducing the community size and range of problems that can be addressed by a particular stack. SOSA tools are either fully OS independent or they support Linux (Intel or ARM), Mac (Intel or M1/M2), and Windows. Similarly, some problems can be addressed orders of magnitude faster on a general-purpose graphics processing unit (GPU) than on a conventional CPU, yet many researchers do not have access to GPUs or are working on problems not suited for them, and so it is essential that a general-purpose software stack support GPU and CPU usage as appropriate. Many SOSA tools provide support for GPUs where appropriate.
% "restricts usage to either cloud or local usage," - didn't get this

\subsubsection{Scriptable}
Some (typically commercial) scientific tools require a GUI, which can be convenient for exploration but makes it difficult to capture a reproducible set of steps for publication and dissemination, as well as making it painful to run long-running jobs or large parameter searches. SOSA tools never require a local desktop, graphical display, or live interaction, making them fully suitable as a basis for reproducible, large-scale, and long-running research.
% curious: what's the culprit here? excel?

\subsubsection{Compositional}
When approaching a particular task, a researcher can either choose a monolithic tool that addresses all aspects of the task, or they can compose lower-level tools that together accomplish the task. Monolithic tools are attractive if they fully cover the use case, but given the dynamic nature of research, it is not reasonable to expect a monolithic tool to cover all of what's needed in any particular research area, let alone across research areas. Accordingly, SOSA tools are all compositional, each meant to be used independently or in combination to solve any particular problem. Of course, compositional approaches can take more initial effort and expertise than an out-of-the box monolithic approach, so for well established complex tasks, a domain-specific monolithic tool can be built out of SOSA tools, leaving other tasks for a compositional approach.
% mention efforts to establish common protocols such as the NEP dunder methods?

\subsubsection{Visualizable}
For computing tasks that complete in the background without any visual output, it is easy to ensure that they are scriptable and cloud friendly. However, doing good science requires understanding each of the processing steps in complete detail, and if there are any unobservable black boxes in your workflows, that is where bugs will hide. To make sure that the \emph{right} work is being done, it is crucial to be able to represent each of the computing steps involved in a way that a human can easily grasp what is happening, with a minimum of extra effort that discourages exploration. Often a remote computing job will export data that is then subsampled and downloaded locally for analysis, but any step that adds friction and covers up the raw data makes it more likely that important issues and insights will be missed. Accordingly, the HoloViz tools included in the SOSA stack are designed to make the full set of data available at any point in the computation, by supporting efficient  visualization of even distributed or GPU-based data of any size using Datashader and hvPlot, assembled from separately computed chunks for display on any device.

\subsubsection{Interactive}
Supporting unattended batch-mode computation is important for doing comprehensive parameter evaluation, but batch runs tend to keep research focused on specific well-trodden paths, changing only a few options at any one time and thus limiting the search space that gets explored for a model or dataset. Using only a batch approach makes it easy to miss important opportunities or to fail to understand major limitations, by simply re-running the same code paths every time. SOSA tools like hvPlot and Panel running in Jupyter make it easy even for remote cloud workflows to be interactive in a web browser, making far more likely that you will understand how it all works and realize all the possibilities.

\subsubsection{Shareable}
Creating easy interactive visualizations is great, but if they are limited to your own desktop or installation, the impact of your ideas and approaches will be limited. With enough HTML/JavaScript/CSS web-technology experience, any computation can be wrapped up as an interactive website, but most scientists are not front-end web developers. The Panel tool in SOSA makes it simple to convert any Jupyter notebook into a web app that can be shared with collaborators or the public to disseminate the results of a project. Panel apps can be shared as static JavaScript-based HTML files (for small datasets), WASM-based HTML files (running Python in the browser), or via a live Python server (for the largest computations and datasets).
% too early for a mention of conda-project?

\subsubsection{OSS}
For software to be fully accessible across years, labs, collaborators, and research problems it is crucial that there be no licensing restrictions that prevent it from being used across the entire discipline and on all relevant hardware and software platforms. SOSA tools are all permissively licensed so that they are freely usable in academia, industry, government, and by private individuals, easily scaled up to the largest problems or in new contexts without having to obtain permission or pay additional fees. They are also all open source, so that if the research hits any fundamental limitations, it is always possible (though not always easy!) to extend or adapt the software for the new requirements.
% but it *IS* much easier toraise an issue and talk directly to developers with OSS!

\subsection{The SOSA stack}

The above considerations determined which libraries are considered to be a part of the SOSA stack. As illustrated in Figure \ref{sosa}, the stack consists of options for each of the major steps in a data-processing task. A finance task might involve files from efficient Parquet-format columnar data storage, accessed from Amazon S3 storage using fsspec file readers, into a Pandas data API, for data processing using Pandas calls plus some custom Numba-optimized analysis code, visualized using an interactive Bokeh plot returned from hvPlot, using Jupyter as a UI (figure \ref{SOSA-finance}).
%%
\begin{figure*}[h]
    \noindent\makebox[\textwidth][c]{\includegraphics[width=0.987\textwidth]{sosa-finance.png}\hfill\hbox{}}
    \caption{SOSA stack applied to a finance problem\label{sosa-finance}}
\end{figure*}
%%
A geoscience task typical for the pangeo.io community might involve files stored in cloud-friendly Zarr multidimensional array storage, accessed using fsspec based on a specification in an Intake data catalog, into a lazy Dask-based xarray multidimensional data structure, with raster procssing done by the Xarray-spatial library (not part of SOSA since it is domain specific, but built using otherwise similar principles), with visualization using xarray's interface to Matplotlib for Dask data structures, and with computation and a UI provided by JupyterHub running in the cloud and providing access to Jupyter.
%%
\begin{figure*}[h]
    \noindent\makebox[\textwidth][c]{\includegraphics[width=\textwidth]{sosa-geoscience.png}}
    \caption{SOSA stack applied to a geoscience problem\label{sosa-geoscience}}
\end{figure*}
%%
As you can see from these examples, any particular problem solved using the SOSA stack can involve completely different underlying libraries, and because each of the SOSA libraries are designed to interoperate freely, users can select the appropriate library or libraries for their needs at each stage. We'll now look into each of the stages in a bit more detail to explain the options available to a researcher using SOSA tools to solve their particular research problems.


\subsubsection{Data storage}

% Add citation for CSV being slow compared to Parquet?
SOSA tools are designed for working with data, whether it comes from a file on disk, a hardware device, remote cloud storage, or a database query. Most existing scientific file formats were designed before cloud computing and are \emph{terrible} choices for efficient distributed computation. The ubiquitous comma-separated-value CSV columnar file format, for instance, is orders of magnitude larger and slower than Parquet, while requiring serial access to the whole file to reach any value in it, thereby preventing parallel reads by processors working on different parts of the task. Even relatively efficient binary formats are typically not ``chunked'' in a way that makes it simple to access the data needed by any particular run or any particular processor in a large compute job.
% seems like an exaggeration. Actually CSV is not always (much) bigger than parquet, it depends. Also, Dask does routinely read it chunkwise
% so long as the first newline in any block is really a record terminator

SOSA tools can provide very efficient, scalable, end-to-end computations only if the data itself is stored in an efficient, chunk-addressable way. Parquet is a suitable well-supported chunked columnar format (for data arranged in rows consisting of differently typed columns), while zarr is a suitable chunked multidimensional array format (for n-dimensional arrays of typed values indexed by row, column, and other dimensions). DuckDB is a relatively new addition to SOSA that makes an underlying file type like CSV or Parquet act like a queryable database, which can be very efficient when accessing small parts or aggregations of large datasets, so other SOSA tools are starting to add support for working with DuckDB as well.

What if you need to access very large collections of data \emph{not} already stored in an efficient chunked format like Parquet or Zarr? Thanks to kerchunk (see Data access below), it is possible to scan such files to record the locations of each chunk of data stored in them, and from then on act as if they underlying file is in Zarr or potentially Parquet format. In this way, SOSA can support access to a wide array of legacy binary file formats such as HDF/NetCDF, GRIB, FITS, and GeoTIFF, which with the appropriate Kerchunk driver can now support efficient scalable computation without having to maintain multiple copies of the underlying data.
% kerchunk does not yet have any parquet workflow

\subsubsection{Data access}

Once you have data in a suitable supported file format, it needs to be located somewhere that SOSA Python code can access it. The fsspec library provides flexible and efficient access to files wherever they might be located, whether that is on your local hard drive, on an FTP file server, on cloud storage like S3, on a website, in a zip file, or in any number of other possible locations. fsspec is now integrated into Pandas and Xarray, transparently providing access from within those libraries (below). It is also supported by Intake, which provides a way to abstract over very large collections of data in different formats to provide a uniform, searchable interface. Intake lets SOSA code be written in a way that does not depend on the location or file type details of the underlying data, making it simpler to write general-purpose, domain-independent code not tied to legacy file formats or readers.
% the bit about Intake sounds weak here. Intake's real strength is in sharing the data definition with others, and of course describing them
% in catalogs that group like things together

As mentioned above, kerchunk can optionally be used for data access to make an older file format be efficiently addressable for SOSA usage, once there are indexing plugins available for that particular (usually domain-specific) format.

\subsubsection{Data API}

At a Python level, most users actually start here, by selecting an an appropriate data application programming interface (API) for their work. Python provides many possible APIs that are suitable for different types of data. Pandas (for columnar data) and Xarray (for multidimensional data) are both supported throughout the SOSA stack, with or without Dask (see below) to handle distributed and out-of-core computation. The other options provide access to other data structures (ragged arrays, for Awkward, and graphs/networks, for GraphBLAS) and/or other computational hardware (RAPIDS and CuPy for GPU architectures), with extensive (but not yet fully complete) support throughout the SOSA stack. Users or domain-specific library authors will typically pick one or more of these data APIs to cover the types of data being used in their domain or their specific problem, and then provide a path to the data to access it using the underlying data-access tools using the data formats and storage available.

\subsubsection{Data processing}

Once the data is accessible from within Python using a data API, the actual computation can begin. Each data API provides a wide variety of inbuilt data-processing routines for selecting, aggregating, and summarizing the data being accessed. Fully custom analysis and data-manipulation code can be implemented directly in Python using Numba, with loops compiled into machine code for speed. To scale to larger problems, Dask can be used to process data in chunks, each on a different core and/or a different machine, with results collected from the underlying processors as needed.

The Datashader library listed in this category is a special case. Datashader is a highly scalable and efficient rasterizer, turning data of any supported type (point, line, region, polygon, raster, etc.) into a regular grid. It is thus suitable as a general-purpose preprocessor for allowing raster/grid-based computation to be done on any type of data. But because regularly gridded rasters directly match the display format for typical devices like monitors and printers, Datashader is also a part of the graphics pipeline for many SOSA applications, and thus also falls under Visualization, below.
% datashader is _mainly_ used for graphics, right?

\subsubsection{Domain-specific}

So far, everything being described has been handled by SOSA libraries, but at this point domain-specific code comes into play. Beyond the generic operations like aggregation and  sampling that apply to any type of data in any field, each scientific area requires specialized code specific to the precise domain being addressed. In some cases, a few loops are all that is needed, and so the general-purpose Numba library is sufficient and can easily be scaled using Dask. In others, it will be sufficient to interface between the data API being used and underlying code in existing libraries, focusing specifically on the domain-specific aspects of that code. Unfortunately, it is not always straightforward to wrap around legacy code of this type without destroying the efficiency and scalablity provided by SOSA, and in such cases the user or author must choose between the difficulty of re-writing that code to support efficient distribution and the lost speed and productivity that would otherwise apply.

Addressing domain-specific code like this is strictly off limits for SOSA, but there are many examples of domain-specific libraries designed to work well with the SOSA tools like Dask and Xarray: xarray-spatial (geoscience), dask-image (image processing), dask-ml (machine learning), icepyx (satellite data), panel-chemistry (chemistry), xdart (x-ray analysis), asari (metabolomics), and megspikes (brain imaging). Tools like this that are fully integrated with the SOSA stack will take time to develop, requiring ambitious projects like building your simulator or analysis tool on top of Dask and xarray or pandas and ensuring that fully visualizable full-scale distributed objects are available for all stages of a workflow. But the focus of such tools can be solely on the aspects specific to a particular research domain, and they can simply inherit all the capabilities of the SOSA stack (now and into the future) for what the SOSA stack covers.

\subsubsection{Visualization}

Whether or not any domain-specific processing has been done, the raw and processed data then need to be visualized so that humans can understand and validate them. Most Python visualization tools (see pyviz.org for a complete list) have limitations that prevent them from being suitable for the SOSA stack. Many of them are limited to relatively small amounts of data, lack support for the various data APIs that are needed, lack support for multidimensional arrays for the fields that need those, or are tied to a desktop OS or GUI. All SOSA tools support basic static-image visualization using Matplotlib, and most also support fully interactive Bokeh- or Plotly-based plotting via hvPlot. hvPlot supports the native `.plot` visualization calls provided by the data APIs, while adding support for efficient server-side distributed rendering using Datashader so that even the largest datasets can be visualized without subsampling or copying the data.

\subsubsection{UI}

The Jupyter Notebook is a ubiquitous domain-independent user interface for working with Python code, and all SOSA tools are fully usable from within Jupyter. Beyond just executing cells full of code, the Panel library in SOSA makes it simple to add interactivity to each Jupyter cell, using a widget to provide control over workflow parameters, and allowing tabular or graphical outputs to be arranged into dashboards or small applications that fit into a cell. Panel also lets users designate certain cells as being ``servable'' if the notebook code is deployed as a separate, standalone application with a UI independent of Jupyter. Panel thus supports the ``interactive'' and ``shareable'' qualities of the SOSA stack, ensuring that your work can have impact on others.

Jupyter focuses on a single researcher or user, but many SOSA-based projects involve collaborations among multiple team members and multiple institutions. Such projects typically use JupyterHub to provide a shared computing environment with the SOSA tools already installed and ready to run. SOSA-based projects often use Nebari (a declarative specification for infrastructure) To simplify configuring JupyterHub, Dask, and associated authentication on cloud servers.

\subsubsection{Environments and reproducibility}

A final category of tools underlying all the rest shown in figure \ref{sosa} centers on how these libraries are packaged and put together into Python environments for solving any particular problem. Many different options are available for Python package and environment management, but SOSA tools are typically used with the conda package and environment manager, because it tracks binary dependencies between libraries, including the underlying C and FORTRAN code that is involved in the domain-specific libraries needed for any particular SOSA workflow. For any particular workflow, once the list of packages needed in the environment has been finalized, the anaconda-project (now being developed in a more general form as conda-project) allows the precise versions involved to be locked on every supported platform, achieving cross-platform reproducibility for a SOSA-based project (see
https://www.anaconda.com/blog/8-levels-of-reproducibility for details).


\subsection{Examples}

To get an understanding of how these tools fit together, there is a variety of example workflows based on SOSA tools available online. The website examples.holoviz.org includes a variety of research topics across domains, with SOSA tools featured prominently in most of them and particularly the Attractors, Census, Ship traffic, and Landsat examples. The holoviz.org site has a tutorial that brings in many of the SOSA packages to solve an example research problem. The Pangeo Project Gallery at pangeo.io and Project Pythia at projectpythia.org both include some very detailed examples in the specific area of climate science. Each SOSA package also has its own documentation and website illustrating what it can do, often in combination with other SOSA tools.
% https://stories.dask.org/en/latest/


\subsection{History and background}

The SOSA libraries individually address important domain-independent problems for scientific research, engineering, and data analysis. As argued above, together they form a coherent and powerful basis for scientific computing in any discipline. Given that each tool has its own developers, separate management structures, and separate communities, how did it happen that together the tools add up to such a coherent approach? To understand this process, it is necessary to dive into the history of some of these projects and the connections between them.

First, many of the SOSA projects were either originally launched at Anaconda, Inc., or they were adapted by people who were at Anaconda at the time to work well with the other projects. Anaconda's consulting division (led by the author Bednar) and open-source tooling division (with most projects led by the author Durant) have worked with a wide variety of government agencies, private foundations, universities, and companies doing numerical computation and research. Each project is designed to address pain points being experienced by those collaborators, and the staff involved help the projects work together to add up to complete solutions.

In particular, the fastparquet, fsspec, kerchunk, intake, numba, dask, datashader, bokeh, hvplot, panel, and conda projects were all created by developers working at Anaconda at the time, with funding from a very wide range of external grants and contracts but with Anaconda playing a central role in developing them and ensuring interoperability between them. These Anaconda-based developers have also contributed extensively to the other projects listed, such as xarray, pandas, awkward-array, graphblas, zarr, and jupyter. Anaconda as a company does not (and cannot!) control the overall set of projects involved, because each has their own contributors, governance structure, and stakeholders. But having a large collection of scientific software developers working at Anaconda on this wide range of projects across many different research fields and domains has led naturally to the emergence of SOSA, to which this paper is only now assigning an name and describing the underlying principles guiding this work.

Out of the many projects that Anaconda developers have been involved in, there is one that deserves a special mention because of how it catalyzed the collection of SOSA tools: Pangeo. The Pangeo.io project is a climate-science initiative based on bringing modern distributed cloud computing approaches to bear on large-scale modeling and analysis of the Earth's climate. Many of the SOSA tools are also Pangeo tools, and the main distinction between SOSA and Pangeo is that Pangeo is domain specific, while SOSA is largely the same stack of tools but explicitly not being tied to any specific research domain. Across numerous grants, contracts, and projects, Pangeo's researchers have improved each of the various tools in the SOSA stack and demonstrated how they can be put together to solve very challenging research problems, cost-effectively processing many terabytes of climate and remote sensing data in a way enabled by SOSA tools like Dask and JupyterHub. This paper is an effort to publicize that the underlying tools are in fact fully domain general and applicable to \emph{all} of science, with different Data API and file format choices but largely the same tools used to cover general data-processing needs that span all research areas.

\subsection{Alternatives to SOSA}

The libraries in SOSA are of course not the only alternatives available in Python; each library individually has alternatives that have their own strengths and advantages. However, the alternative libraries have not (yet?) been integrated with SOSA tools, making it much more difficult to apply them to a SOSA-based project. For instance, Ray is an alternative approach to distributed computation that is not supported by these tools, and so if a project uses Ray to manage distributed computation, then they cannot easily select hvPlot for visualization without first converting the data structures into something hvPlot understands. Similarly, Vaex and Polars offer alternatives to the Pandas/Dask columnar dataframes supported in SOSA, and so projects based on those data APIs will not (yet!) easily be able to use SOSA tools for visualization and user interfaces. There are also now alternative tools for server-side rendering of large datasets that in SOSA are handled by Datashader, such as VegaFusion for Vega and Altair, but those are not fully integrated with the other SOSA libraries like Dask and Numba.
% ray does have an explicit integration with dask. I don't know how much use it gets.

There are also full alternative stacks to consider, such as tools like Hadoop and Spark from the Apache Foundation. Most Apache tools relying on the Java Virtual Machine (JVM) that provides OS-independent computation but requires a heavyweight runtime compared to the SOSA tools, making it awkward to combine most Apache tools with SOSA tools. SOSA already offers flexible support for distributed computation without the JVM overhead, making ``big data'' tools ike Hadoop and Spark unnecessary for SOSA applications. SOSA does rely on the low-level Parquet and Arrow Apache projects, which have non-JVM implementations that fit well into the SOSA stack.

\subsubsection{Future work}
This paper is the first to describe SOSA as an entity or collection of tools. The specific tools we selected are those that in the opinion of the authors are well integrated with the other tools and provide a ``mix and match'' approach to putting together libraries to implement a particular analysis workflow. There is not currently any entity besides these authors and this paper that defines what SOSA is, which libraries are included, and what process to follow for a library to be included or excluded from SOSA. As SOSA evolves, it will be important to formalize some of these processes, similar to how the Pangeo organization and the HoloViz organization (of which hvPlot and Panel are a part) have been constituted with a steering committee and governance policies. Community sites like Discourse or Slack or Discord that let researchers using SOSA communicate and collaborate will also be essential for establishing a SOSA community that crosses research areas and domains.

Until then, SOSA is just a concept and an explanation for the work of hundreds of people working mostly independently but with key connections that make these independent projects come together into a coherent stack for performing scientific research.

Other relevant areas that need work are the development of domain-general tools for capturing metadata, conventions, consistent units, etc. per domain (like the CF conventions for climate science), improved tools for collaboration (implementing part of the JupyterHub roadmap (https://jupyterhub.readthedocs.io/en/stable/contributing/roadmap.html), and
% and?

\subsubsection{Conclusion}

The SOSA stack is ready to use today, as an extensive basis for scientific computing in any research area and across many different communities. There are alternatives for each of the components of the SOSA stack, but the advantage of having this very wide array of functionality that works well together is that researchers in any particular domain can just get on with their actual work in that domain, freed from having to reimplement basic data handling in all its forms and freed from the limitations of legacy domain-specific stacks. Everything involved is open source, so feel free to use any of these tools in any combination to solve any problems that you have!

